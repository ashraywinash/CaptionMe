{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08648bed-fa2a-4457-aad5-3750b0385463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenizer_file': 'tokeniser_1.json', 'batchSize': 8}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from config import getConfig\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available else \"cpu\"\n",
    "torch.device = device\n",
    "config = getConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf741d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c984ff4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['captions', 'Unnamed: 1'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the results.xl file and test\n",
    "\n",
    "results = pd.read_excel(\"./flickr8k_images/captions.xls\")\n",
    "results.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = results[results.columns[1]].apply(lambda x: not isinstance(x,float))\n",
    "results.columns = [\"image\",\"caption\"]\n",
    "results = results[mask]\n",
    "\n",
    "results[results.columns[1]]=results[results.columns[1]].apply(lambda x: x.strip())\n",
    "\n",
    "results = results.iloc[1:,:]\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if Data is Correctly Downloaded\n",
    "def testImage():\n",
    "    randNo = random.randint(0,10000)\n",
    "\n",
    "    IMAGE_PATH = \"./flickr8k_images/flickr8k_images/\"+results[results.columns[0]][randNo]\n",
    "\n",
    "    imageTransforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = plt.imread(IMAGE_PATH)\n",
    "    image = imageTransforms(image)\n",
    "\n",
    "    plt.title(results[results.columns[1]][randNo])\n",
    "    plt.imshow(torch.transpose(image,0,2).numpy())\n",
    "    plt.show()\n",
    "\n",
    "    return image\n",
    "\n",
    "image = testImage()\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b792b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text tokeniser\n",
    "# Build Vocabulary\n",
    "\n",
    "\n",
    "\n",
    "def get_tokeniser(ds):\n",
    "\n",
    "    def get_all_sentences(ids):\n",
    "        for text in ids[\"caption\"]:\n",
    "            if type(text) == float:\n",
    "                continue\n",
    "            else:\n",
    "                yield text\n",
    "\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"])\n",
    "\n",
    "    if tokenizer_path.exists():\n",
    "\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "        return tokenizer\n",
    "\n",
    "    else:\n",
    "\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "        return tokenizer\n",
    "    \n",
    "tokenizer = get_tokeniser(results)\n",
    "\n",
    "print(tokenizer.encode(\"My name is Ashrya Shravan\").ids)\n",
    "print(tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76255d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b56ce551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets and Data loaders for images\n",
    "\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, ds: pd.DataFrame, transforms: torchvision.transforms, tokenizer_tgt : Tokenizer, seq_len : torch.int64 = 100) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "\n",
    "        # Image reading and transforms\n",
    "        image = plt.imread(\"./flickr8k_images/flickr8k_images/\"+self.ds.iloc[index,:][\"image\"])\n",
    "        encoder_input = self.transforms(image)\n",
    "\n",
    "        # text transforms\n",
    "        tgt_text = self.ds.iloc[index,:][\"caption\"]\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return encoder_input, decoder_input, label \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "imageTransforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1976143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 7, 7])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Model Declaration and Image Passing to get Features from Image\n",
    "\n",
    "class ImageCaptionCNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        f\"\"\"\n",
    "            This requires input of format [N,C,H,W] with all transforms\n",
    "        \"\"\"\n",
    "\n",
    "        self.vgg16 = torchvision.models.vgg16(weights=\"DEFAULT\")\n",
    "\n",
    "        self.VGGLAYER = nn.Sequential(*self.vgg16.features.children())\n",
    "\n",
    "        self.Pool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        image = self.VGGLAYER(image)\n",
    "\n",
    "        image = self.Pool(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "\n",
    "\n",
    "imageTransforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "Image_model = ImageCaptionCNNModel()\n",
    "\n",
    "res = Image_model(image)\n",
    "\n",
    "res.size() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b009c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model: torch.int64 = 512,vocab_size:torch.int64=5439,*args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "\n",
    "        self.cnn_layer = ImageCaptionCNNModel()\n",
    "\n",
    "        for param in self.cnn_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.d_model)\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model)\n",
    "\n",
    "        self.transformer_layer = nn.Transformer(batch_first=True)\n",
    "        \n",
    "        self.projection_layer = nn.Sequential(\n",
    "            nn.Linear(self.d_model,self.vocab_size)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self,image,token):\n",
    "\n",
    "        image = self.cnn_layer(image) #[4096]\n",
    "        image = image.view(image.size()[0],-1,512)\n",
    "\n",
    "        embeddings = self.embedding(token) *  math.sqrt(self.d_model)\n",
    "\n",
    "        image = self.transformer_layer(image,embeddings)\n",
    "\n",
    "        image = self.projection_layer(image)\n",
    "\n",
    "\n",
    "\n",
    "        return image\n",
    "        \n",
    "\n",
    "model = ImageCaptioningModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The model\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "global_step = 0\n",
    "writer = SummaryWriter(\"ImageCaptioning_2\")\n",
    "\n",
    "tokenizer = get_tokeniser(results)\n",
    "\n",
    "imageDataset = ImageCaptionDataset(results,imageTransforms,tokenizer)\n",
    "\n",
    "imagedataLoader = DataLoader(imageDataset,BATCH_SIZE)\n",
    "\n",
    "\n",
    "model = ImageCaptioningModel().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'),label_smoothing=0.1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, eps=1e-9)\n",
    "\n",
    "\n",
    "batch_iterator = imagedataLoader\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idx,batch in enumerate(batch_iterator):\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        proj_out = model(batch[0].to(device),batch[1].to(device)) \n",
    "\n",
    "        label = batch[2].to(device)\n",
    "\n",
    "        loss = loss_fn(proj_out.view(-1, tokenizer.get_vocab_size()), label.view(-1))\n",
    "\n",
    "        writer.add_scalar('train loss', loss.item(), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if(idx%1000 == 0):\n",
    "            print(f\"Completed {idx} batches at loss {loss}\")\n",
    "\n",
    "    resDir = Path('results')\n",
    "    finalPath = resDir / f\"naive{epoch}.pth\"\n",
    "    torch.save(model.state_dict(),finalPath)\n",
    "    print(f\"Completed {epoch} epoch with loss {loss} and saved model\")\n",
    "\n",
    "\n",
    "    # model.eval()\n",
    "\n",
    "    # with torch.inference_mode():\n",
    "    #     res = model(batch[0],batch[1])\n",
    "\n",
    "    # print(res.size())\n",
    "    # if(idx>=10):\n",
    "    #     break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c392f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(),finalPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resDir = Path('results')\n",
    "finalPath = resDir / 'naive9.pth'\n",
    "captioner = ImageCaptioningModel().to(device)\n",
    "\n",
    "captioner.load_state_dict(torch.load(finalPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, imageTransforms):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = imageTransforms(image).unsqueeze(0)  \n",
    "    return image\n",
    "\n",
    "def generate_caption(model, tokenizer, image, max_length=100, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        image = image.to(device)\n",
    "\n",
    "        caption_tokens = [tokenizer.token_to_id('[SOS]')]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "\n",
    "            input_tokens = torch.tensor(caption_tokens).unsqueeze(0).to(device)  \n",
    "            \n",
    "            output = model(image, input_tokens)\n",
    "            print(output.shape)\n",
    "            next_token_logits = output[0, -1, :]  \n",
    "            print(next_token_logits.shape)\n",
    "            next_token_id = next_token_logits.argmax(dim=-1).item()  \n",
    "            print(next_token_id)\n",
    "\n",
    "            caption_tokens.append(next_token_id)\n",
    "            \n",
    "            if next_token_id == tokenizer.token_to_id('[EOS]'):\n",
    "                break\n",
    "\n",
    "    caption = [tokenizer.id_to_token(token_id) for token_id in caption_tokens]\n",
    "\n",
    "    print(caption)\n",
    "\n",
    "    caption = caption[1:-1]\n",
    "    \n",
    "    caption = ' '.join(caption)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "\n",
    "image_path = './flickr30k_images/flickr30k_images/36979.jpg'\n",
    "image = preprocess_image(image_path, imageTransforms)\n",
    "\n",
    "caption = generate_caption(captioner, tokenizer, image, device=device)\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3801b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
